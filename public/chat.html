<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Agentic System - Chat</title>
  <style>
    * { box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
      margin: 0;
      background: #f7f7f9;
      display: flex;
      flex-direction: column;
      height: 100vh;
    }
    header {
      background: #2563eb;
      color: white;
      padding: 16px 24px;
      display: flex;
      justify-content: space-between;
      align-items: center;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    header h1 {
      margin: 0;
      font-size: 20px;
      font-weight: 600;
    }
    .settings-btn {
      background: rgba(255,255,255,0.2);
      border: 1px solid rgba(255,255,255,0.3);
      color: white;
      padding: 8px 16px;
      border-radius: 6px;
      cursor: pointer;
      font-size: 14px;
      transition: background 0.3s;
    }
    .settings-btn:hover {
      background: rgba(255,255,255,0.3);
    }
    .chat-container {
      flex: 1;
      display: flex;
      flex-direction: column;
      max-width: 1000px;
      width: 100%;
      margin: 0 auto;
      padding: 24px;
      overflow: hidden;
    }
    #transcript {
      flex: 1;
      overflow-y: auto;
      padding: 20px;
      background: white;
      border-radius: 12px;
      margin-bottom: 16px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .msg {
      margin-bottom: 20px;
      display: flex;
      flex-direction: column;
    }
    .msg.user {
      align-items: flex-end;
    }
    .msg.assistant {
      align-items: flex-start;
    }
    .bubble {
      max-width: 70%;
      padding: 12px 16px;
      border-radius: 18px;
      word-wrap: break-word;
      line-height: 1.5;
    }
    .user .bubble {
      background: #2563eb;
      color: white;
      border-bottom-right-radius: 4px;
    }
    .assistant .bubble {
      background: #e5e7eb;
      color: #111;
      border-bottom-left-radius: 4px;
    }
    .role {
      font-size: 12px;
      color: #6b7280;
      margin-bottom: 4px;
      font-weight: 600;
    }
    .input-area {
      display: flex;
      gap: 12px;
      background: white;
      padding: 16px;
      border-radius: 12px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    #input {
      flex: 1;
      padding: 12px 16px;
      border: 2px solid #e5e7eb;
      border-radius: 24px;
      font-size: 14px;
      resize: none;
      font-family: inherit;
      max-height: 120px;
    }
    #input:focus {
      outline: none;
      border-color: #2563eb;
    }
    #send {
      padding: 12px 24px;
      background: #2563eb;
      color: white;
      border: none;
      border-radius: 24px;
      font-size: 14px;
      font-weight: 600;
      cursor: pointer;
      transition: background 0.3s;
    }
    #send:hover:not(:disabled) {
      background: #1d4ed8;
    }
    #send:disabled {
      background: #9ca3af;
      cursor: not-allowed;
    }
    .loading {
      display: inline-block;
      width: 16px;
      height: 16px;
      border: 2px solid #e5e7eb;
      border-top-color: #2563eb;
      border-radius: 50%;
      animation: spin 0.6s linear infinite;
    }
    @keyframes spin {
      to { transform: rotate(360deg); }
    }
    .empty-state {
      text-align: center;
      color: #6b7280;
      padding: 60px 20px;
    }
    .empty-state h2 {
      color: #374151;
      margin-bottom: 8px;
    }
  </style>
</head>
<body>
  <header>
    <h1>Agentic System Chat</h1>
    <button class="settings-btn" onclick="window.location.href='/setup.html'">⚙️ Settings</button>
  </header>
  
  <div class="chat-container">
    <div id="transcript">
      <div class="empty-state">
        <h2>Welcome!</h2>
        <p>Start a conversation with your AI assistant</p>
      </div>
    </div>
    
    <div class="input-area">
      <textarea id="input" placeholder="Type your message..." rows="1"></textarea>
      <button id="send" type="button">Send</button>
      <button id="start-voice" type="button">Start Voice</button>
      <button id="stop-voice" type="button" disabled>Stop Voice</button>
      <label class="checkbox-group"><input type="checkbox" id="voice-respond"> Voice reply on finalize</label>
      <span id="voice-state">Idle</span>
      <button id="start-asr" type="button">Start ASR Test</button>
      <button id="stop-asr" type="button" disabled>Stop ASR Test</button>
      <button id="vad-test" type="button" disabled>Run VAD on Last</button>
    </div>
    <div id="asr-result" class="empty-state"></div>
    <div id="vad-result" class="empty-state"></div>
  </div>

  <script>
    const transcriptEl = document.getElementById('transcript');
    const inputEl = document.getElementById('input');
    const sendBtn = document.getElementById('send');
    const startVoiceBtn = document.getElementById('start-voice');
    const stopVoiceBtn = document.getElementById('stop-voice');
    const startAsrBtn = document.getElementById('start-asr');
    const stopAsrBtn = document.getElementById('stop-asr');
    const asrResultEl = document.getElementById('asr-result');
    const vadBtn = document.getElementById('vad-test');
    const vadResultEl = document.getElementById('vad-result');
    const voiceRespondEl = document.getElementById('voice-respond');
    const voiceStateEl = document.getElementById('voice-state');
    
    let messages = [];
    let turn = 0;
    let mediaRecorder = null;
    let audioStream = null;
    let isPlayingAudio = false;
    let currentAudio = null;
    let sessionId = null;
    let chunks = [];
    let asrRecorder = null;
    let asrSessionId = null;
    let asrChunks = [];
    let webrtcWs = null;  // WebRTC WebSocket connection

    // Auto-resize textarea
    inputEl.addEventListener('input', function() {
      this.style.height = 'auto';
      this.style.height = Math.min(this.scrollHeight, 120) + 'px';
    });

    // Send on Enter (Shift+Enter for new line)
    inputEl.addEventListener('keydown', function(e) {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        sendMessage();
      }
    });

    sendBtn.addEventListener('click', sendMessage);
    startVoiceBtn.addEventListener('click', startVoice);
    stopVoiceBtn.addEventListener('click', stopVoice);
    startAsrBtn.addEventListener('click', startAsr);
    stopAsrBtn.addEventListener('click', stopAsr);
    vadBtn.addEventListener('click', runVad);

    function render() {
      if (messages.length === 0) {
        transcriptEl.innerHTML = `
          <div class="empty-state">
            <h2>Welcome!</h2>
            <p>Start a conversation with your AI assistant</p>
          </div>
        `;
        return;
      }

      transcriptEl.innerHTML = '';
      messages.forEach(m => {
        if (m.role === 'system') return;
        
        const div = document.createElement('div');
        div.className = `msg ${m.role}`;
        
        const role = document.createElement('div');
        role.className = 'role';
        role.textContent = m.role === 'user' ? 'You' : 'Assistant';
        
        const bubble = document.createElement('div');
        bubble.className = 'bubble';
        bubble.textContent = m.content;
        
        div.appendChild(role);
        div.appendChild(bubble);
        transcriptEl.appendChild(div);
      });
      
      transcriptEl.scrollTop = transcriptEl.scrollHeight;
    }

    async function sendMessage() {
      const text = inputEl.value.trim();
      if (!text) return;
      
      inputEl.value = '';
      inputEl.style.height = 'auto';
      
      messages.push({ role: 'user', content: text });
      render();
      
      sendBtn.disabled = true;
      sendBtn.innerHTML = '<span class="loading"></span>';
      
      try {
        const res = await fetch('/api/scripted_chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            content: text,
            turn: turn,
            history: messages.slice(-8)
          })
        });
        
        const data = await res.json();
        
        if (data.reply) {
          messages.push({ role: 'assistant', content: data.reply });
        } else {
          messages.push({ role: 'assistant', content: 'Sorry, I encountered an error.' });
        }
        
        render();
        turn++;
      } catch (err) {
        messages.push({ role: 'assistant', content: 'Error: ' + err.message });
        render();
      } finally {
        sendBtn.disabled = false;
        sendBtn.textContent = 'Send';
      }
    }

    // Check if setup is complete
    async function checkSetup() {
      try {
        const res = await fetch('/api/setup/status');
        const data = await res.json();
        if (!data.configured) {
          if (confirm('Setup not completed. Would you like to configure the system now?')) {
            window.location.href = '/setup.html';
          }
        }
      } catch (e) {
        console.error('Failed to check setup status:', e);
      }
    }

    checkSetup();
    
    async function startVoice() {
      try {
        // Start WebRTC session
        const res = await fetch('/api/voice/webrtc/start', { method: 'POST' });
        const data = await res.json();
        if (!data.ok) throw new Error('Failed to start WebRTC session');
        sessionId = data.session_id;
        
        // Connect WebSocket for WebRTC signaling and data
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${window.location.host}/api/voice/webrtc/${sessionId}`;
        webrtcWs = new WebSocket(wsUrl);
        
        webrtcWs.onopen = () => {
          console.log('WebRTC WebSocket connected');
          voiceStateEl.textContent = 'Connecting…';
          
          // Get audio stream
          navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
            audioStream = stream; // Store stream reference for muting/unmuting
            const opts = { mimeType: 'audio/webm;codecs=opus', audioBitsPerSecond: 128000 };
            mediaRecorder = new MediaRecorder(stream, opts);
            
            mediaRecorder.ondataavailable = async (e) => {
              if (e.data && e.data.size > 0 && webrtcWs && webrtcWs.readyState === WebSocket.OPEN && !isPlayingAudio) {
                // Only send audio chunks when not playing audio (prevent feedback loop)
                // Convert blob to base64 and send via WebSocket
                const reader = new FileReader();
                reader.onload = () => {
                  const base64 = reader.result.split(',')[1]; // Remove data:audio/webm;base64, prefix
                  webrtcWs.send(JSON.stringify({
                    type: 'audio_chunk',
                    data: base64,
                    respond: voiceRespondEl.checked
                  }));
                };
                reader.readAsDataURL(e.data);
              }
            };
            
            mediaRecorder.start(300); // Send chunks every 300ms
            voiceStateEl.textContent = 'Listening…';
            startVoiceBtn.disabled = true;
            stopVoiceBtn.disabled = false;
          }).catch(err => {
            console.error('Failed to get audio stream:', err);
            alert('Failed to access microphone: ' + err.message);
            webrtcWs.close();
          });
        };
        
        webrtcWs.onmessage = (event) => {
          try {
            const msg = JSON.parse(event.data);
            console.log('WebSocket message received:', msg);
            
            if (msg.type === 'processing_result') {
              if (msg.state) {
                voiceStateEl.textContent = msg.state === 'recording' ? 'Recording…' : 'Listening…';
              }
              
              if (msg.finalized) {
                // Silence detected - process the buffered audio
                console.log('Finalized message:', { transcript: msg.transcript, reply: msg.reply });
                if (msg.transcript && msg.transcript.trim()) {
                  messages.push({ role: 'user', content: msg.transcript.trim() });
                  console.log('Added user message:', msg.transcript.trim());
                } else {
                  console.warn('No transcript in finalized message');
                }
                if (msg.reply && msg.reply.trim()) {
                  messages.push({ role: 'assistant', content: msg.reply.trim() });
                  console.log('Added assistant message:', msg.reply.trim());
                }
                render();
                voiceStateEl.textContent = 'Listening…';
              }
            } else if (msg.type === 'audio_ready' && msg.audio_path) {
              // Play TTS audio response
              // Stop any currently playing audio
              if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
              }
              
              // Build audio URL with audio_file parameter if provided
              let audioUrl = msg.audio_path;
              if (msg.audio_file) {
                const url = new URL(audioUrl, window.location.origin);
                url.searchParams.set('audio_file', encodeURIComponent(msg.audio_file));
                audioUrl = url.pathname + url.search;
              }
              
              // Prevent recording during playback to avoid feedback loop
              isPlayingAudio = true;
              // Mute audio tracks to prevent feedback (chunks won't be sent due to isPlayingAudio check)
              if (audioStream) {
                audioStream.getAudioTracks().forEach(track => {
                  track.enabled = false;
                });
              }
              
              voiceStateEl.textContent = 'Speaking…';
              
              const audio = new Audio(audioUrl);
              currentAudio = audio;
              
              const resumeRecording = () => {
                isPlayingAudio = false;
                // Unmute audio tracks
                if (audioStream) {
                  audioStream.getAudioTracks().forEach(track => {
                    track.enabled = true;
                  });
                }
                voiceStateEl.textContent = 'Listening…';
                currentAudio = null;
              };
              
              audio.onended = () => {
                // Resume recording after audio finishes
                resumeRecording();
                // Notify server that playback is complete
                if (webrtcWs && webrtcWs.readyState === WebSocket.OPEN) {
                  webrtcWs.send(JSON.stringify({ type: 'playback_complete' }));
                }
              };
              
              audio.onerror = (err) => {
                console.error('Audio play failed:', err);
                // Resume recording even on error
                resumeRecording();
                // Notify server that playback failed/ended
                if (webrtcWs && webrtcWs.readyState === WebSocket.OPEN) {
                  webrtcWs.send(JSON.stringify({ type: 'playback_complete' }));
                }
              };
              
              audio.play().catch(err => {
                console.error('Audio play failed:', err);
                // Resume recording on play error
                resumeRecording();
                // Notify server that playback failed
                if (webrtcWs && webrtcWs.readyState === WebSocket.OPEN) {
                  webrtcWs.send(JSON.stringify({ type: 'playback_complete' }));
                }
              });
            } else if (msg.type === 'error') {
              console.error('WebRTC error:', msg.error);
              voiceStateEl.textContent = 'Error';
            }
          } catch (err) {
            console.error('Error parsing WebSocket message:', err);
          }
        };
        
        webrtcWs.onerror = (error) => {
          console.error('WebRTC WebSocket error:', error);
          voiceStateEl.textContent = 'Connection Error';
        };
        
        webrtcWs.onclose = () => {
          console.log('WebRTC WebSocket closed');
          if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
          }
          webrtcWs = null;
        };
        
      } catch (e) {
        alert(e.message);
        if (webrtcWs) webrtcWs.close();
      }
    }

    async function stopVoice() {
      try {
        // Stop MediaRecorder
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
          await new Promise((resolve) => {
            mediaRecorder.addEventListener('stop', resolve, { once: true });
            mediaRecorder.stop();
          });
        }
        
        // Close WebSocket connection
        if (webrtcWs) {
          webrtcWs.close();
          webrtcWs = null;
        }
        
        // Stop audio tracks
        if (audioStream) {
          audioStream.getTracks().forEach(track => track.stop());
          audioStream = null;
        }
        // Stop any playing audio
        if (currentAudio) {
          currentAudio.pause();
          currentAudio = null;
        }
        isPlayingAudio = false;
      } catch (e) {
        console.error('Error stopping voice:', e);
      } finally {
        startVoiceBtn.disabled = false;
        stopVoiceBtn.disabled = true;
        mediaRecorder = null;
        sessionId = null;
        voiceStateEl.textContent = 'Idle';
      }
    }

    async function startAsr() {
      try {
        const res = await fetch('/api/voice/start', { method: 'POST' });
        const data = await res.json();
        if (!data.ok) throw new Error('Failed to start ASR');
        asrSessionId = data.session_id;
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const opts = { mimeType: 'audio/webm;codecs=opus', audioBitsPerSecond: 128000 };
        asrRecorder = new MediaRecorder(stream, opts);
        asrChunks = [];
        asrRecorder.ondataavailable = (e) => { if (e.data.size > 0) asrChunks.push(e.data); };
        asrRecorder.start(200);
        startAsrBtn.disabled = true;
        stopAsrBtn.disabled = false;
        asrResultEl.textContent = '';
      } catch (e) {
        alert(e.message);
      }
    }

    async function stopAsr() {
      try {
        if (asrRecorder) {
          await new Promise((resolve) => {
            asrRecorder.addEventListener('stop', resolve, { once: true });
            asrRecorder.stop();
          });
        }
        const blob = new Blob(asrChunks, { type: 'audio/webm' });
        const fd = new FormData();
        fd.append('audio', blob, 'input.webm');
        const up = await fetch(`/api/voice/upload?session_id=${encodeURIComponent(asrSessionId)}`, { method: 'POST', body: fd });
        const upData = await up.json();
        if (!upData.ok) throw new Error('Upload failed');
        vadBtn.disabled = false;
        const st = await fetch('/api/voice/asr', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ session_id: asrSessionId }) });
        const stData = await st.json();
        if (stData.ok) {
          asrResultEl.textContent = stData.transcript || '(no audio recognized)';
        } else {
          alert('ASR failed');
        }
      } catch (e) {
        alert(e.message);
      } finally {
        startAsrBtn.disabled = false;
        stopAsrBtn.disabled = true;
        asrRecorder = null;
        asrChunks = [];
      }
    }

    async function runVad() {
      try {
        if (!asrSessionId) {
          alert('No session');
          return;
        }
        vadResultEl.textContent = 'Processing VAD...';
        const res = await fetch('/api/voice/vad', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ session_id: asrSessionId }) });
        const data = await res.json();
        console.log('VAD response:', data);
        if (data.ok) {
          const segs = data.segments || [];
          if (segs.length === 0) {
            vadResultEl.textContent = 'VAD Segments: (none detected)';
          } else {
            const lines = ['VAD Segments:'];
            for (const s of segs) {
              const start = typeof s.start === 'number' ? s.start : parseFloat(s.start);
              const end = typeof s.end === 'number' ? s.end : parseFloat(s.end);
              if (!isNaN(start) && !isNaN(end)) {
                lines.push(`- ${start.toFixed(2)}s → ${end.toFixed(2)}s (${(end - start).toFixed(2)}s)`);
              } else {
                lines.push(`- Invalid segment: ${JSON.stringify(s)}`);
              }
            }
            vadResultEl.textContent = lines.join('\n');
          }
        } else {
          vadResultEl.textContent = `VAD failed: ${data.error || 'Unknown error'}`;
          alert('VAD failed: ' + (data.error || 'Unknown error'));
        }
      } catch (e) {
        vadResultEl.textContent = `Error: ${String(e)}`;
        alert(String(e));
      }
    }
  </script>
</body>
</html>

